{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP76fijZdiUR30/Fd0ZI3cG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41371113h-xian/114-1/blob/main/hw_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "#  PTT NBA爬蟲 + Gradio互動介面（最完整Colab版本）\n",
        "# ==========================================================\n",
        "!pip -q install -U gradio==4.* requests beautifulsoup4 pandas openpyxl\n",
        "\n",
        "import os, re, io, time, json, shutil, random, requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import gradio as gr\n",
        "\n",
        "BASE_URL = \"https://www.ptt.cc\"\n",
        "\n",
        "# -----------------------------\n",
        "# Session 與 Request設定\n",
        "# -----------------------------\n",
        "def make_session():\n",
        "    s = requests.Session()\n",
        "    s.cookies.set('over18', '1', domain='ptt.cc')\n",
        "    s.headers.update({\n",
        "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36\"\n",
        "    })\n",
        "    return s\n",
        "\n",
        "def http_get(s, url, retries=3, delay=1.2):\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            r = s.get(url, timeout=10)\n",
        "            if r.status_code == 200:\n",
        "                return r\n",
        "        except:\n",
        "            time.sleep(delay)\n",
        "    return None\n",
        "\n",
        "def get_index(s, board=\"NBA\"):\n",
        "    url = f\"{BASE_URL}/bbs/{board}/index.html\"\n",
        "    r = http_get(s, url)\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "    up = soup.find(\"a\", string=\"上頁 ›\")\n",
        "    if not up:\n",
        "        up = soup.find(\"a\", string=re.compile(\"上頁\"))\n",
        "    idx = int(re.search(r\"index(\\d+)\\.html\", up[\"href\"]).group(1)) + 1\n",
        "    return idx\n",
        "\n",
        "# -----------------------------\n",
        "# 抓取索引頁文章列表\n",
        "# -----------------------------\n",
        "def parse_index(html):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    rows = soup.find_all(\"div\", class_=\"r-ent\")\n",
        "    data = []\n",
        "    for r in rows:\n",
        "        tdiv = r.find(\"div\", class_=\"title\")\n",
        "        a = tdiv.find(\"a\") if tdiv else None\n",
        "        title = a.text.strip() if a else tdiv.text.strip()\n",
        "        link = BASE_URL + a['href'] if a else None\n",
        "        author = r.find(\"div\", class_=\"author\").text.strip()\n",
        "        date = r.find(\"div\", class_=\"date\").text.strip()\n",
        "        nrec = r.find(\"div\", class_=\"nrec\").text.strip()\n",
        "        nrec = 100 if nrec == \"爆\" else (int(nrec[1:])*-1 if nrec.startswith(\"X\") else int(nrec) if nrec.isdigit() else 0)\n",
        "        data.append({\"title\": title, \"link\": link, \"author\": author, \"nrec\": nrec, \"date\": date})\n",
        "    return data\n",
        "\n",
        "# -----------------------------\n",
        "# 抓取文章內文與推文\n",
        "# -----------------------------\n",
        "def parse_article(html):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    meta = {m.select_one('.article-meta-tag').text: m.select_one('.article-meta-value').text\n",
        "            for m in soup.select('.article-metaline') if m.select_one('.article-meta-tag')}\n",
        "    title = meta.get(\"標題\", \"\")\n",
        "    author = meta.get(\"作者\", \"\")\n",
        "    time_str = meta.get(\"時間\", \"\")\n",
        "\n",
        "    main = soup.find(id=\"main-content\")\n",
        "    for t in main.select('.article-metaline, .article-metaline-right'): t.extract()\n",
        "    content = main.get_text(\"\\n\", strip=False).split('--\\n')[0].strip()\n",
        "\n",
        "    pushes = []\n",
        "    for p in soup.select('.push'):\n",
        "        tag = p.select_one('.push-tag').text.strip()\n",
        "        user = p.select_one('.push-userid').text.strip()\n",
        "        cont = p.select_one('.push-content').text.strip(':').strip()\n",
        "        iptime = p.select_one('.push-ipdatetime').text.strip()\n",
        "        pushes.append({\"tag\": tag, \"user\": user, \"content\": cont, \"time\": iptime})\n",
        "    return title, author, time_str, content, pushes\n",
        "\n",
        "# -----------------------------\n",
        "# 主爬蟲邏輯\n",
        "# -----------------------------\n",
        "def scrape_ptt(board=\"NBA\", pages=5, delay=1.0, fetch_detail=False, progress=gr.Progress(track_tqdm=True)):\n",
        "    s = make_session()\n",
        "    cur = get_index(s, board)\n",
        "    results, pushes_all = [], []\n",
        "\n",
        "    for i in progress.tqdm(range(cur, cur-pages, -1), desc=\"抓取索引頁\"):\n",
        "        html = http_get(s, f\"{BASE_URL}/bbs/{board}/index{i}.html\")\n",
        "        if not html: continue\n",
        "        data = parse_index(html.text)\n",
        "        for d in data:\n",
        "            d[\"board\"], d[\"page_index\"] = board, i\n",
        "        results.extend(data)\n",
        "        time.sleep(delay)\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    if fetch_detail:\n",
        "        for i, row in progress.tqdm(df.iterrows(), total=len(df), desc=\"抓取文章內容\"):\n",
        "            if not row.link: continue\n",
        "            art = http_get(s, row.link)\n",
        "            if not art: continue\n",
        "            title, author, time_full, content, pushes = parse_article(art.text)\n",
        "            df.loc[i, \"title_full\"] = title\n",
        "            df.loc[i, \"author_full\"] = author\n",
        "            df.loc[i, \"time_full\"] = time_full\n",
        "            df.loc[i, \"content\"] = content\n",
        "            for p in pushes:\n",
        "                p[\"title\"], p[\"board\"] = title, board\n",
        "            pushes_all.extend(pushes)\n",
        "            time.sleep(delay)\n",
        "\n",
        "    # 儲存檔案\n",
        "    os.makedirs(\"/content/ptt_data\", exist_ok=True)\n",
        "    df.to_csv(f\"/content/ptt_data/{board}_articles.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "    if pushes_all:\n",
        "        pd.DataFrame(pushes_all).to_csv(f\"/content/ptt_data/{board}_pushes.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "    return df, pushes_all\n",
        "\n",
        "# -----------------------------\n",
        "# Gradio 介面\n",
        "# -----------------------------\n",
        "def run_scraper(board, pages, delay, fetch_detail):\n",
        "    df, pushes = scrape_ptt(board, pages, delay, fetch_detail)\n",
        "    msg = f\"✅ 已完成爬取 {board} 看板，共 {len(df)} 筆文章\"\n",
        "    if fetch_detail:\n",
        "        msg += f\"，共擷取 {len(pushes)} 筆推文。\"\n",
        "    msg += \"\\n檔案已儲存於 /content/ptt_data/\"\n",
        "    return df.head(300), msg\n",
        "\n",
        "with gr.Blocks(title=\"PTT NBA爬蟲（Colab最完整版本）\") as demo:\n",
        "    gr.Markdown(\"## 🏀 PTT 爬蟲互動介面（Colab版）\\n可抓取標題、作者、連結、推文數與內文。\")\n",
        "    board = gr.Textbox(value=\"NBA\", label=\"看板名稱\")\n",
        "    pages = gr.Slider(1, 30, value=5, step=1, label=\"抓取頁數（最新往前）\")\n",
        "    delay = gr.Slider(0.5, 3.0, value=1.0, step=0.1, label=\"每頁延遲（秒）\")\n",
        "    fetch_detail = gr.Checkbox(value=False, label=\"是否抓取文章內文與推文\")\n",
        "    btn = gr.Button(\"開始爬取 🚀\")\n",
        "    output_df = gr.Dataframe(label=\"文章清單（前300筆）\")\n",
        "    output_msg = gr.Markdown()\n",
        "    btn.click(fn=run_scraper, inputs=[board, pages, delay, fetch_detail], outputs=[output_df, output_msg])\n",
        "\n",
        "# ✅ Colab啟動設定（會在cell中內嵌顯示並提供分享連結）\n",
        "demo.queue()\n",
        "demo.launch(inline=True, share=True, server_name=\"0.0.0.0\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "CbvhONL1WaUC",
        "outputId": "0e0c8a92-6930-43b4-812a-3aa3eb647fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://6469d899f4c781febe.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6469d899f4c781febe.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}